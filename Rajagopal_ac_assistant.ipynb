{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75617fbc",
   "metadata": {},
   "source": [
    "# AC Recommendation assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a778a",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "We have a dataset of Air Conditioners with all their specifications. The goal is to build a chatbot that helps the user to buy the right product based on their requirements. The chatbot is expected to recommend suitable options and guide the user in making the purchse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42b021",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6fa419",
   "metadata": {},
   "source": [
    "#### Installing the relevant packages such as openai, tenacity, gdown, dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d413d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q openai tenacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e04c81",
   "metadata": {},
   "source": [
    "#### **Loading** the **environment variables** here with the use of dotenv. It basically loads the variables from **.env file**. So, please create a .env file with **OPENAI_API_KEY=sk_** before executing the following line. Replace sk_ with your secret key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e22a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddccb33",
   "metadata": {},
   "source": [
    "##### **Dataset gets downloaded** from my google drive since this is just a single ipynb file. Sample dataset is faster in execution, so it is set by default. Please change it to complete dataset where you could expect some latency in getting the response.\n",
    "complete dataset - \"ac_data.csv\" \n",
    "\n",
    "sample dataset - \"ac_data_only_top_30.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83843645",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "# Sample or test dataset: totally 30 \n",
    "file_id = \"1rgfu_8CHtMNB4304cu3yZQyVBq0qIWmw\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "filename_samples = \"ac_data_only_top_30.csv\"\n",
    "gdown.download(url, filename_samples, quiet=False)\n",
    "\n",
    "\"\"\"\n",
    "# Complete dataset: totally 100 \n",
    "file_id = \"16eprE-VtKJ5V1WwTW7ylCk7rCaaGfLB4\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "filename_complete = \"ac_data.csv\"\n",
    "gdown.download(url, filename_complete, quiet=False)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a3cc7",
   "metadata": {},
   "source": [
    "#### load the dataset into a dataframe **df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv(filename_samples)\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef84a4",
   "metadata": {},
   "source": [
    "## 2. Utilities and helper functions\n",
    "\n",
    "This section contains the utility functions that could be used through out the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4315ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModerationException(Exception):\n",
    "    \"\"\"This is a custom exception class to handle moderation related exceptions\"\"\"\n",
    "\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_system_message(template_path: str, **params):\n",
    "    \"\"\"\n",
    "    Reads the markdown template and renders it with named placeholders.\n",
    "\n",
    "    :param template_path str: Path of the MD file\n",
    "    :param **params str: Positional parameters that can be embedded into the system message\n",
    "    \"\"\"\n",
    "    path = Path(template_path)\n",
    "    system_message = path.read_text(encoding=\"utf-8\")\n",
    "    try:\n",
    "        return system_message.format(**params)\n",
    "    except KeyError as key_error:\n",
    "        missing = key_error.args\n",
    "        raise KeyError(f\"Missing template parameter: '{missing}' for {path}\")\n",
    "\n",
    "\n",
    "def load_system_message_without_params(template_path: str):\n",
    "    \"\"\"\n",
    "    Reads the markdown template and renders it without any paramete.\n",
    "\n",
    "    :param template_path str: Path of the MD file\n",
    "    \"\"\"\n",
    "    path = Path(template_path)\n",
    "    return path.read_text(\"utf-8\")\n",
    "\n",
    "\n",
    "def load_dictionary_from_md(template_path: str):\n",
    "    \"\"\"\n",
    "    Reads the markdown template and renders the dictionary.\n",
    "\n",
    "    :param template_path str: Path of the MD file\n",
    "    \"\"\"\n",
    "    with open(template_path) as data:\n",
    "        return ast.literal_eval(data.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0966c490",
   "metadata": {},
   "source": [
    "## 3. Model layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102abd2",
   "metadata": {},
   "source": [
    "#### Moderation check API interface and implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Moderation(ABC):\n",
    "    @abstractmethod\n",
    "    def get_response(self, input_message) -> bool:\n",
    "        \"\"\"\n",
    "        Invokes OpenAI moderation check API and returns either the message is flagged or not.\n",
    "\n",
    "        :param str input_message: Input message under check.\n",
    "        :return bool: returns True if there is a sensitive/violent content, otherwise False\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8173350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
    "class ModerationModel(Moderation):\n",
    "    def __init__(self):\n",
    "        self.__client = OpenAI()\n",
    "\n",
    "    def get_response(self, input_message) -> bool:\n",
    "        response = self.__client.moderations.create(input=input_message)\n",
    "        return response.results[0].flagged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e9648",
   "metadata": {},
   "source": [
    "#### Chat completion API interface and implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f84281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ChatModel(ABC):\n",
    "    @abstractmethod\n",
    "    def get_session_response(\n",
    "        self, json_format: bool = False, tools: Dict = None, tool_choice=None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Return the response using the accumulated messages of a session and it returns the LLM's response.\n",
    "\n",
    "        :param bool json_format: True if response_format is json object False otherwise.\n",
    "        :param Dict tools: function calling dictionary to set the list of tools. None by default.\n",
    "        :param Dict tool_choice: function calling dictionary to enforce a particular tool/function. None by default.\n",
    "        :return: list of responses\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def preview_response(\n",
    "        self, messages: List[Dict], json_format: bool = False\n",
    "    ) -> List[str | Dict]:\n",
    "        \"\"\"\n",
    "        Stateless, one-time request from the given messages.\n",
    "        This method does not read or modify the internal session.\n",
    "\n",
    "        :param List[Dict] messages: List of messages to be passed to the model.\n",
    "        :param bool json_format: True if response_format is json object False otherwise.\n",
    "        :return: list of responses\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_parameters(\n",
    "        self,\n",
    "        max_tokens: int = 100,\n",
    "        model: str = \"gpt-3.5-turbo\",\n",
    "        no_of_choices: int = 1,\n",
    "        temperature: float = 0,\n",
    "        seed=None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        To update the model parameters.\n",
    "\n",
    "        :param int max_tokens: Updates the maximum number of output tokens to produce.\n",
    "        :param str model: Updates the model\n",
    "        :param int no_of_choices: Toatl number of answers required from the model\n",
    "        :param float temperature: This parameter decides how random the output should be generated by the model.\n",
    "        :param int seed: To get the most deterministic output from an LLM on different API calls to introduce consistency.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_message(\n",
    "        self,\n",
    "        role: str,\n",
    "        content: str = None,\n",
    "        name: str = None,\n",
    "        tool_calls: List[Dict] = None,\n",
    "        tool_call_id: str = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        To add a message in messages history.\n",
    "\n",
    "        :param str role: The roles of the message such as system, user, and assistant\n",
    "        :param str content: The system message or user message to be sent\n",
    "        :param str name: str or None. An example can be set i.e. example_user informs the model that this is an example.\n",
    "        :param List[Dict] tool_calls: To add the tool calls response\n",
    "        :param str tool_call_id: To include the respective tool id for the LLM to identify the exact tool in messages.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def clear_messages(self) -> None:\n",
    "        \"\"\"\n",
    "        To clear the messages history since the chat can go on forever with the user.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, final\n",
    "\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "\n",
    "\n",
    "@final\n",
    "class OpenAIChatModel(ChatModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        max_tokens: int,\n",
    "        temperature: float,\n",
    "        no_of_choices: int,\n",
    "        seed: int,\n",
    "    ):\n",
    "        self.__client = OpenAI()\n",
    "        self.__model = model\n",
    "        self.__temperature = temperature\n",
    "        self.__no_of_choices = no_of_choices\n",
    "        self.__max_tokens = max_tokens\n",
    "        self.__seed = seed\n",
    "        self.__messages = []\n",
    "\n",
    "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
    "    def get_session_response(\n",
    "        self, json_format=False, json_schema=None, tools=None, tool_choice: Dict = None\n",
    "    ) -> List[str]:\n",
    "        if len(self.__messages) == 0:\n",
    "            raise Exception(\"Please add a message to start with the conversation!\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.__model,\n",
    "            \"n\": self.__no_of_choices,\n",
    "            \"max_tokens\": self.__max_tokens,\n",
    "            \"temperature\": self.__temperature,\n",
    "            \"messages\": self.__messages,\n",
    "            \"seed\": self.__seed,\n",
    "        }\n",
    "        if json_format:\n",
    "            params[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "        elif json_schema is not None:\n",
    "            params[\"response_format\"] = json_schema\n",
    "        elif tools is not None:\n",
    "            params[\"tools\"] = [tools]\n",
    "            params[\"tool_choice\"] = tool_choice\n",
    "\n",
    "        response = self.__client.chat.completions.create(**params)\n",
    "\n",
    "        if json_format == True:\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "        else:\n",
    "            return self.__parse_response(response=response)\n",
    "\n",
    "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
    "    def preview_response(\n",
    "        self, messages: List[Dict], json_format=False, json_schema=None\n",
    "    ) -> List[str | Dict]:\n",
    "        if len(messages) == 0:\n",
    "            raise Exception(\"Please add a message to start with the conversation!\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.__model,\n",
    "            \"n\": self.__no_of_choices,\n",
    "            \"max_tokens\": self.__max_tokens,\n",
    "            \"temperature\": self.__temperature,\n",
    "            \"messages\": messages,\n",
    "            \"seed\": self.__seed,\n",
    "        }\n",
    "\n",
    "        if json_format:\n",
    "            params[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "        elif json_schema is not None:\n",
    "            params[\"response_format\"] = json_schema\n",
    "\n",
    "        response = self.__client.chat.completions.create(**params)\n",
    "\n",
    "        if json_format == True:\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "        else:\n",
    "            return [choice.message.content for choice in response.choices]\n",
    "\n",
    "    def update_parameters(\n",
    "        self,\n",
    "        max_tokens: int = 100,\n",
    "        model: str = \"gpt-3.5-turbo\",\n",
    "        no_of_choices: int = 1,\n",
    "        temperature: float = 0,\n",
    "        seed=None,\n",
    "    ) -> None:\n",
    "        self.__max_tokens = max_tokens\n",
    "        self.__model = model\n",
    "        self.__no_of_choices = no_of_choices\n",
    "        self.__temperature = temperature\n",
    "        self.__seed = seed\n",
    "\n",
    "    def add_message(\n",
    "        self,\n",
    "        role: str,\n",
    "        content: str = None,\n",
    "        name: str = None,\n",
    "        tool_calls: List[Dict] = None,\n",
    "        tool_call_id: str = None,\n",
    "    ) -> None:\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        if name is not None:\n",
    "            message[\"name\"] = name\n",
    "            message[\"role\"] = \"system\"\n",
    "        elif tool_call_id is not None:\n",
    "            message[\"tool_call_id\"] = tool_call_id\n",
    "        elif tool_calls is not None:\n",
    "            message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "        self.__messages.append(message)\n",
    "\n",
    "    def clear_messages(self):\n",
    "        self.__messages = []\n",
    "\n",
    "    def __parse_response(self, response) -> List:\n",
    "        \"\"\"\n",
    "        This method extracts the necessary information from a normal or tools response for further processing.\n",
    "\n",
    "        :param List[Dict] response: Dictionary that can contain a normal or tools response.\n",
    "        :return List[Dict|str]: Parses into List[Dict] for tools response, and List[str] for normal response.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for choice in response.choices:\n",
    "            if choice.message.tool_calls:\n",
    "                for tool_call in choice.message.tool_calls:\n",
    "                    args = json.loads(tool_call.function.arguments)\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"id\": tool_call.id,\n",
    "                            \"content\": args,\n",
    "                            \"tool_calls\": response.choices[0].message.tool_calls,\n",
    "                        }\n",
    "                    )\n",
    "            elif choice.message.content:\n",
    "                results.append(choice.message.content)\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7f506",
   "metadata": {},
   "source": [
    "## 4. Stages implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad61b20",
   "metadata": {},
   "source": [
    "#### Compare two feature dictionaries in the product extraction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def compare_products(user_requirements: Dict, from_dataset: Dict) -> int:\n",
    "    \"\"\"\n",
    "    This method compares the dataset row and the user's requirements dictionary\n",
    "    by mapping categorical values, then computes the final correspondence score.\n",
    "\n",
    "    :param Dict user_requirements: Dictionary that contains the actual user requirements\n",
    "    :param Dict from_dataset: Extracted featues dictionary from the actual dataset (one sample/row)\n",
    "    :return int: Returns the final score after the comparison\n",
    "    \"\"\"\n",
    "    mapping = {\"essential\": 0, \"standard\": 1, \"premium\": 2}\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    for key, value in from_dataset.items():\n",
    "        user_value = mapping.get(user_requirements[key], -1)\n",
    "        dataset_value = mapping.get(value, -1)\n",
    "\n",
    "        if (dataset_value != -1 and user_value != -1) and dataset_value >= user_value:\n",
    "            score += 1\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff67d95e",
   "metadata": {},
   "source": [
    "#### Stage 0 Implementation - initialize conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class StageZeroResult:\n",
    "    response: List[str]\n",
    "\n",
    "\n",
    "class IntializeConversation:\n",
    "    def __init__(self, chat_model: ChatModel, system_message: str):\n",
    "        self.__chat_model = chat_model\n",
    "        self.system_message = system_message\n",
    "\n",
    "    def run(self) -> StageZeroResult:\n",
    "        \"\"\"\n",
    "        This method generates the first welcome message to the user by using an LLM API call and initiates the conversation.\n",
    "\n",
    "        :return StageZeroResult: Returns the LLM's response with the welcome message.\n",
    "        \"\"\"\n",
    "        response = self.__chat_model.get_session_response()\n",
    "\n",
    "        return StageZeroResult(response=response)\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"\n",
    "        To add a message in messages history.\n",
    "\n",
    "        :param str role: The roles of the message such as system, user, or assistant\n",
    "        :param str content: The system message or user message to be sent\n",
    "        :param name\n",
    "        \"\"\"\n",
    "        self.__chat_model.add_message(role=role, content=content)\n",
    "\n",
    "    def clear_messages(self):\n",
    "        \"\"\"\n",
    "        To clear the messages history of this particular stage since the chat can go on forever with the user.\n",
    "        \"\"\"\n",
    "        self.__chat_model.clear_messages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aaf7e3",
   "metadata": {},
   "source": [
    "#### Stage 1 implementation - Intent clarification, extraction and confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "@dataclass\n",
    "class StageOneResult:\n",
    "    intent_confirmation: str  # Yes or No response\n",
    "    response: str\n",
    "    user_requirements: Dict\n",
    "\n",
    "\n",
    "class IntentClarityAndConfirmation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chat_model: ChatModel,\n",
    "        classify_values_system_message: str,\n",
    "        intent_confirmation_system_message: str,\n",
    "        extract_dict_system_message: str,\n",
    "        function_tool: Dict,\n",
    "        function_tool_choice: Dict,\n",
    "    ):\n",
    "        self.__chat_model = chat_model\n",
    "        self.__classify_values_system_message = classify_values_system_message\n",
    "        self.__intent_confirmation_system_message = intent_confirmation_system_message\n",
    "        self.__extract_dict_system_message = extract_dict_system_message\n",
    "        self.__function_tool = function_tool\n",
    "        self.__function_tool_choice = function_tool_choice\n",
    "        self.__initial_requirements = {\n",
    "            \"price\": \"-\",\n",
    "            \"energy efficiency\": \"-\",\n",
    "            \"cooling capacity\": \"-\",\n",
    "            \"comfort\": \"-\",\n",
    "            \"ac type\": \"-\",\n",
    "            \"smart features\": \"-\",\n",
    "            \"portability\": \"-\",\n",
    "        }\n",
    "        self.__user_requirements_dictionary = self.__initial_requirements.copy()\n",
    "\n",
    "    def run(self) -> StageOneResult:\n",
    "        \"\"\"\n",
    "        This method runs the intent clarification and confirmation layer and results the user requirements dictionary.\n",
    "\n",
    "        1. Invoking the function calling API all to extract requirements from users input\n",
    "        2. Handling the tools response by extracting and adding the tool messages to model's conversation history\n",
    "        3. Adding extracted user requirements dictionary in the conversation\n",
    "        4. Convert the user requirements i.e. 1.5 tons, fixed unit etc. to its respective values such as essential, standard, or premium\n",
    "        5. Intent confirmation layer, outputs \"Yes\" if all requirements are collected otherwise \"No\" and clarification continues\n",
    "        6. If intent is confirmed extacts the final user requirements dictionary from the string.\n",
    "        7. Returns the stage one result for both intent confirmation \"Yes\" and for \"No\"\n",
    "\n",
    "        :param StageOneResult: Returns the stage one result with the user requirements dictionary upon intent confirmation.\n",
    "        \"\"\"\n",
    "        # 1). Invoking the function calling API all to extract requirements from users input\n",
    "        tool_response = self.__chat_model.get_session_response(\n",
    "            tools=self.__function_tool, tool_choice=self.__function_tool_choice\n",
    "        )\n",
    "\n",
    "        # 2). Handling the tools response by extracting and adding the tool messages to model's conversation history\n",
    "        self.__handle_tool_response(tool_response)\n",
    "\n",
    "        # 3). Adding extracted user requirements dictionary in the conversation\n",
    "        self.__chat_model.add_message(\n",
    "            role=\"assistant\",\n",
    "            content=self.__classify_values_system_message.format(\n",
    "                json.dumps(self.__user_requirements_dictionary)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # 4). Convert the user requirements i.e. 1.5 tons, fixed unit etc. to its respective values such as essential, standard, or premium\n",
    "        response = self.__chat_model.get_session_response()\n",
    "\n",
    "        # 5). Intent confirmation layer, outputs \"Yes\" if all requirements are collected otherwise \"No\" and clarification continues\n",
    "        intent_confirmation = self.__intent_confirmation(response[0])\n",
    "\n",
    "        # 6). If intent is confirmed extacts the final user requirements dictionary from the string.\n",
    "        user_requirement_dict = {}\n",
    "        if intent_confirmation[0].lower() == \"yes\":\n",
    "            user_requirement_dict = self.__extract_dict_from_string(response[0])\n",
    "            self.__user_requirements_dictionary = self.__initial_requirements.copy()\n",
    "\n",
    "        # 7). Returns the stage one result for both intent confirmation \"Yes\" and for \"No\"\n",
    "        return StageOneResult(\n",
    "            intent_confirmation=intent_confirmation[0],\n",
    "            response=response[0],\n",
    "            user_requirements=user_requirement_dict,\n",
    "        )\n",
    "\n",
    "    def __intent_confirmation(self, model_response: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        One time OpenAI API call to get intent confirmation with the following steps.\n",
    "        1. System prompt message and user mesasge is added to messages\n",
    "        2. One time API call to get the model ersponse\n",
    "\n",
    "        :param str model_response: Response for the API call with function tools to extract features from user input\n",
    "        :return List[str]: Returns a single choice response with \"Yes\" or \"No\" based on collected features so far\n",
    "        \"\"\"\n",
    "        # 1). System prompt message is added to messages\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.__intent_confirmation_system_message,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"Here is the input: {model_response}\"},\n",
    "        ]\n",
    "\n",
    "        # 2). One time API call to get the model ersponse\n",
    "        completion_response = self.__chat_model.preview_response(messages=messages)\n",
    "\n",
    "        return completion_response\n",
    "\n",
    "    def __extract_dict_from_string(self, response: str):\n",
    "        \"\"\"\n",
    "        Extracts the features dictionary from string upon intent confirmation\n",
    "\n",
    "        1. System prompt message and user mesasge is added to messages\n",
    "        2. Returns one time API call to get the features dictionary\n",
    "        \"\"\"\n",
    "        # 1). System prompt message and user mesasge is added to messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.__extract_dict_system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"Here is the input: {response}\"},\n",
    "        ]\n",
    "\n",
    "        # 2). Returns one time API call to get the features dictionary\n",
    "        response_dict = self.__chat_model.preview_response(\n",
    "            messages=messages, json_format=True\n",
    "        )\n",
    "\n",
    "        return response_dict\n",
    "\n",
    "    def __handle_tool_response(self, tool_response):\n",
    "        \"\"\"\n",
    "        This method handles the function tool response to extract features and add it to conversation history.\n",
    "\n",
    "        1. Access tool_calls, directly with tool_response[0].tool_calls[0] here, but can be accessed like a dictionary too\n",
    "        2. Access function, directly with tool_call.function here, but can be accessed like a dictionary too\n",
    "        3. Access arguments, directly with .function.arguments here, but can be access as dictionary too\n",
    "        4. Store all the values to local dictionary after parsing\n",
    "        5. Appending tools message from last api call before appending the tool response itself, otherwise LLM wouldn't understand the tool execution\n",
    "\n",
    "        :param str model_response: Response for the API call with function tools to extract features from user input\n",
    "        \"\"\"\n",
    "        # 1). Access tool_calls, directly with tool_response[0].tool_calls[0] here, but can be accessed like a dictionary too\n",
    "        tool_call = (\n",
    "            tool_response[0][\"tool_calls\"][0]\n",
    "            if isinstance(tool_response[0], dict)\n",
    "            else tool_response[0].tool_calls[0]\n",
    "        )\n",
    "        # 2). Access function, It can be accessed directly with tool_call.function here, but can be accessed like a Dict too\n",
    "        func = (\n",
    "            tool_call[\"function\"] if isinstance(tool_call, dict) else tool_call.function\n",
    "        )\n",
    "        # 3). Access arguments, It can be accessed directly with .function.arguments here, but can be access as Dict too\n",
    "        arguments_json = func[\"arguments\"] if isinstance(func, dict) else func.arguments\n",
    "        # Parse only if it's a string\n",
    "        if isinstance(arguments_json, str):\n",
    "            arguments_json = json.loads(arguments_json)\n",
    "\n",
    "        # 4). Store all the values to local dictionary after parsing\n",
    "        for key, value in arguments_json.items():\n",
    "            self.__user_requirements_dictionary[key] = value\n",
    "\n",
    "        # 5). Appending tools message from last api call before appending the tool response itself, otherwise LLM wouldn't understand the tool execution\n",
    "        self.__chat_model.add_message(\n",
    "            role=\"assistant\", tool_calls=tool_response[0][\"tool_calls\"]\n",
    "        )\n",
    "        self.__chat_model.add_message(\n",
    "            role=\"tool\",\n",
    "            tool_call_id=tool_response[0][\"id\"],\n",
    "            content=json.dumps(tool_response[0][\"content\"]),\n",
    "        )\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"\n",
    "        To add a message in messages history.\n",
    "\n",
    "        :param str role: The roles of the message such as system, user, or assistant\n",
    "        :param str content: The system message or user message to be sent\n",
    "        \"\"\"\n",
    "        self.__chat_model.add_message(role=role, content=content)\n",
    "\n",
    "    def clear_messages(self):\n",
    "        \"\"\"\n",
    "        To clear the messages history of this particular stage since the chat can go on forever with the user.\n",
    "        \"\"\"\n",
    "        self.__chat_model.clear_messages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291206c",
   "metadata": {},
   "source": [
    "#### Stage 2 implementation - product extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class StageTwoResult:\n",
    "    recommendations: str\n",
    "\n",
    "\n",
    "class ProductExtractionAndMapping:\n",
    "    def __init__(self, chat_model: ChatModel, system_message: str):\n",
    "        self.__chat_model = chat_model\n",
    "        self.__system_message = system_message\n",
    "        self.__df = pd.read_csv(filename_samples)\n",
    "        self.__VALIDATION_SCORE = 3\n",
    "\n",
    "    def run(self, user_requirement: Dict) -> StageTwoResult:\n",
    "        \"\"\"\n",
    "        This method runs the product extraction and mapping layer with the following steps\n",
    "        1. Extracting price that matches user's budget\n",
    "        2. Extracting features dictionary from product description via mapping layer and stores it in a new column\n",
    "        3. Computing scores by comparing user requirement and dataset row dictionaries and stores it in scores column\n",
    "        4. Sorts and Filters the products based on top scores\n",
    "        5. Sorts top products based on price, then drops unwanted columns for the next conversation\n",
    "\n",
    "        :param Dict user_requirement: User's requirement dictionary from the previous stage\n",
    "        :param StageTwoResult: Returns product recommendations in a json string format\n",
    "        \"\"\"\n",
    "        # 1). Extract products within user's expected price range\n",
    "        filtered_df = self.__df[\n",
    "            self.__df[\"price\"]\n",
    "            <= int(round(float(user_requirement[\"price\"].strip().replace(\",\", \"\"))))\n",
    "        ]\n",
    "\n",
    "        # 2). Features extracted via mapping layer and stored in a separate column as a new feature\n",
    "        filtered_df[\"ac_features\"] = filtered_df[\"description\"].apply(\n",
    "            lambda description: self.__product_map_layer(description=description)\n",
    "        )\n",
    "\n",
    "        # 3). Calling compare products method for the scores\n",
    "        filtered_df[\"scores\"] = filtered_df[\"ac_features\"].apply(\n",
    "            lambda dataset: compare_products(\n",
    "                user_requirements=user_requirement, from_dataset=dataset\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 4). Sorts and filters the top products after validation\n",
    "        top_products = filtered_df.sort_values(\"scores\", ascending=False).head(3)\n",
    "        top_products = top_products[top_products[\"scores\"] > self.__VALIDATION_SCORE]\n",
    "\n",
    "        # 5). Sorts top products based on price, then drops unwanted columns for the next conversation\n",
    "        top_products.sort_values(\"price\", ascending=False, inplace=True)\n",
    "        top_products.drop(columns=[\"scores\", \"ac_features\"], axis=1, inplace=True)\n",
    "\n",
    "        return StageTwoResult(recommendations=top_products.to_json(orient=\"records\"))\n",
    "\n",
    "    def __product_map_layer(self, description) -> Dict:\n",
    "        \"\"\"\n",
    "        Private method receives the product description, where an LLM extracts all the primary features from it,\n",
    "        then classifies the values into one of the following categories such as essential, standard, or premium.\n",
    "\n",
    "        :param str description: Product description that contains all the specifications\n",
    "        :return Dict: Returns the extracted featues as a dictionary\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.__system_message.format(description)},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Follow the instructions and output the dictionary in json format for the following AC {description}\",\n",
    "            },\n",
    "        ]\n",
    "        response = self.__chat_model.preview_response(\n",
    "            messages=messages, json_format=True\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"\n",
    "        To add a message in messages history.\n",
    "\n",
    "        :param str role: The roles of the message such as system, user, or assistant\n",
    "        :param str content: The system message or user message to be sent\n",
    "        \"\"\"\n",
    "        self.__chat_model.add_message(role=role, content=content)\n",
    "\n",
    "    def clear_messages(self):\n",
    "        \"\"\"\n",
    "        To clear the messages history of this particular stage since the chat can go on forever with the user.\n",
    "        \"\"\"\n",
    "        self.__chat_model.clear_messages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7234658",
   "metadata": {},
   "source": [
    "#### Stage 3 implementation - product recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class StageThreeResult:\n",
    "    response: List[str]\n",
    "\n",
    "\n",
    "class ProductRecommendations:\n",
    "    def __init__(self, chat_model: ChatModel, system_message: str):\n",
    "        self.__chat_model = chat_model\n",
    "        self.system_message = system_message\n",
    "\n",
    "    def run(self) -> StageThreeResult:\n",
    "        \"\"\"\n",
    "        This method actually runs the final stage of the pipeling\n",
    "\n",
    "        :return StageThreeResult: Data class that contains the recommendations\n",
    "        \"\"\"\n",
    "        recommendation = self.__chat_model.get_session_response()\n",
    "\n",
    "        return StageThreeResult(response=recommendation)\n",
    "\n",
    "    def continue_run(self) -> StageThreeResult:\n",
    "        \"\"\"\n",
    "        This method responds to user query through an LLM API call.\n",
    "\n",
    "        :return StageThreeResult: Data class that contains the response\n",
    "        \"\"\"\n",
    "        response = self.__chat_model.get_session_response()\n",
    "\n",
    "        return StageThreeResult(response=response)\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"\n",
    "        To add a message in messages history.\n",
    "\n",
    "        :param str role: The roles of the message such as system, user, or assistant\n",
    "        :param str content: The system message or user message to be sent\n",
    "        \"\"\"\n",
    "        self.__chat_model.add_message(role, content)\n",
    "\n",
    "    def clear_messages(self):\n",
    "        \"\"\"\n",
    "        To clear the messages history of this particular stage since the chat can go on forever with the user.\n",
    "        \"\"\"\n",
    "        self.__chat_model.clear_messages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced526b",
   "metadata": {},
   "source": [
    "## 5. Pipeline Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Orchestrates the workflow of the project by running different stages.\n",
    "\n",
    "    This pipeline is responsible for the following\n",
    "    1. Adding user and assistant messages to chat model's message history.\n",
    "    2. Moderation checks and throws an exception in case of failure\n",
    "    3. Internal processing happens on individual stages where each stage is responsible for updating the message history i.e.\n",
    "        * In stage 1, Function calling tool response.\n",
    "        * In stage 2, One time responses such as extacting features dictionary from description\n",
    "    4. System messages are kept in individual stages rather than putting all in one place\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        moderation_client: Moderation,\n",
    "        stage0: IntializeConversation,\n",
    "        stage1: IntentClarityAndConfirmation,\n",
    "        stage2: ProductExtractionAndMapping,\n",
    "        stage3: ProductRecommendations,\n",
    "    ):\n",
    "        self.__moderation_client = moderation_client\n",
    "        self.__stage0 = stage0\n",
    "        self.__stage1 = stage1\n",
    "        self.__stage2 = stage2\n",
    "        self.__stage3 = stage3\n",
    "        self.__ERROR_MESSAGE = \"This conversation ends now since your input has some sensitive content. Please start the new conversation to continue!\"\n",
    "\n",
    "    def run_stage0(self) -> StageZeroResult:\n",
    "        \"\"\"\n",
    "        Initializes the conversation with the user by invoking stage 0, where a system message is added.\n",
    "\n",
    "        :return StageZeroResult: Returns stage 0 result.\n",
    "        \"\"\"\n",
    "        # 1). Adds system message to model's message history\n",
    "        self.__stage0.add_message(role=\"system\", content=self.__stage0.system_message)\n",
    "\n",
    "        # 2). Runs stage zero\n",
    "        response = self.__stage0.run()\n",
    "\n",
    "        return response\n",
    "\n",
    "    def run_stage1(self, user_input: str) -> StageOneResult:\n",
    "        \"\"\"\n",
    "        Runs stage 1 for intent clarification and confirmation, and outputs user requirements dictionary. The following steps are executed\n",
    "\n",
    "        1. Moderation check on user input\n",
    "        2. Adds user input to the message history\n",
    "        3. Runs stage 1\n",
    "        4. Executing the moderation check on LLM's output\n",
    "        5. Add LLM's response to message history if some requirements are still missing\n",
    "\n",
    "        :param str user_input: User's input message\n",
    "        :return StageOneResult: Returns stage 1 starts running clarification loop until user requirements dictionary is extracted.\n",
    "        :raises ModerationException: Throws the custom exception\n",
    "        \"\"\"\n",
    "        # 1). Moderation check on user input\n",
    "        self.__moderation_check(user_input)\n",
    "\n",
    "        # 2). Adds user input to message history\n",
    "        self.__stage1.add_message(\"user\", user_input)\n",
    "\n",
    "        # 3). Runs stage 1\n",
    "        stage1_response = self.__stage1.run()\n",
    "\n",
    "        # 4). Moderation check on LLM's response\n",
    "        self.__moderation_check(stage1_response.response)\n",
    "\n",
    "        # 5). If intent clarification is not complete, add the assistant message to the message history\n",
    "        if stage1_response.intent_confirmation.strip() == \"No\":\n",
    "            self.__stage1.add_message(\"assistant\", stage1_response.response)\n",
    "\n",
    "        return stage1_response\n",
    "\n",
    "    def run_stage2(self, user_requirement: Dict) -> StageTwoResult:\n",
    "        \"\"\"\n",
    "        Runs stage 2 for intent clarification and confirmation, and outputs user requirements dictionary. The following steps are executed\n",
    "\n",
    "        :param Dict user_requirement: Collected user requirement's dictionary\n",
    "        :return StageTwoResult: Returns stage 2 result with the product recommendations.\n",
    "        :raises ModerationException: Throws the custom exception\n",
    "        \"\"\"\n",
    "        # 1). Runs stage 2\n",
    "        response = self.__stage2.run(user_requirement=user_requirement)\n",
    "\n",
    "        # 2). Moderation check on the LLM's response\n",
    "        self.__moderation_check(response.recommendations)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def run_stage3(self, recommendations) -> StageThreeResult:\n",
    "        \"\"\"\n",
    "        Runs stage 3 by showing the recommendations to the user. The following steps are executed\n",
    "\n",
    "        1. Appending user requirements dictionary to system message, then add it to message history\n",
    "        2. Start running stage 2 by generating recommendations that can be displayed to the user\n",
    "        3. Executing the moderation check on LLM's output\n",
    "        4. Add recommendations and assistant's response in previous step to message history\n",
    "\n",
    "        :param Dict: User's input message\n",
    "        :return StageThreeResult: Returns stage 1 result with the user requirements dictionary.\n",
    "        :raises ModerationException: Throws the custom exception\n",
    "        \"\"\"\n",
    "        # 1). Appending user requirements to system message and add it to message history\n",
    "        conversation_recommendation = self.__stage3.system_message.format(\n",
    "            recommendations\n",
    "        )\n",
    "        self.__stage3.add_message(\"system\", conversation_recommendation)\n",
    "\n",
    "        # 2). Run stage 3\n",
    "        recommendation = self.__stage3.run()\n",
    "\n",
    "        # 3). Moderation check on LLM's response\n",
    "        self.__moderation_check(recommendation.response)\n",
    "\n",
    "        # 4). Add recommendations and assistant's response in previous step to message history\n",
    "        self.__stage3.add_message(\"user\", \"This is my user profile\" + recommendations)\n",
    "        self.__stage3.add_message(\"assistant\", \"\\n\".join(recommendation.response))\n",
    "\n",
    "        return recommendation\n",
    "\n",
    "    def continue_stage3(self, user_input: str) -> StageThreeResult:\n",
    "        \"\"\"\n",
    "        Continues to run stage 3 by keeping the user engaged with doubt resolution session.\n",
    "\n",
    "        1. Adds user message to message history of the model\n",
    "        2. Moderation check on user input\n",
    "        3. Continues to run stage 3\n",
    "        4. Moderation check on LLM's response\n",
    "        5. Add's LLM response to model's message history\n",
    "\n",
    "        :param Dict: User's input message\n",
    "        :return StageOneResult: Returns stage 1 result with the user requirements dictionary.\n",
    "        :raises ModerationException: Throws the custom exception\n",
    "        \"\"\"\n",
    "        # 1). Adds user message to message history\n",
    "        self.__stage3.add_message(\"user\", user_input)\n",
    "\n",
    "        # 2). Moderation check on user input\n",
    "        self.__moderation_check(user_input)\n",
    "\n",
    "        # 3). Continue running stage 3\n",
    "        response = self.__stage3.continue_run()\n",
    "\n",
    "        # 4). Moderation check on LLM' output\n",
    "        response_str = \"\\n\".join(response.response)\n",
    "        self.__moderation_check(response_str)\n",
    "\n",
    "        # 5). Add assistant message to message history\n",
    "        self.__stage3.add_message(\"assistant\", \"\\n\".join(response_str))\n",
    "\n",
    "        return response\n",
    "\n",
    "    def __moderation_check(self, data):\n",
    "        \"\"\"\n",
    "        Runs the moderation check and returns the flag\n",
    "\n",
    "        :param str data: Data on which the moderation check\n",
    "        :raises ModerationException: Throws the custom exception\n",
    "        \"\"\"\n",
    "        flagged = self.__moderation_client.get_response(data)\n",
    "\n",
    "        if flagged:\n",
    "            raise ModerationException(self.__ERROR_MESSAGE)\n",
    "\n",
    "    def clear_messages(self):\n",
    "        \"\"\"\n",
    "        Clears the chat message history on all stages.\n",
    "        \"\"\"\n",
    "        self.__stage0.clear_messages()\n",
    "        self.__stage1.clear_messages()\n",
    "        self.__stage2.clear_messages()\n",
    "        self.__stage3.clear_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90bb4a",
   "metadata": {},
   "source": [
    "## 6. Prompts & function tools\n",
    "\n",
    "Prompts are separated since it gets injected via dependency injection framework to the modules for better testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e8525",
   "metadata": {},
   "source": [
    "### Stage 0 Intent clarification layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aeba91",
   "metadata": {},
   "source": [
    "#### Intent clarification system message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE0_SYSTEM_MESSAGE = \"\"\"\n",
    "Act as a smart Air Conditioner recommendation assistant specializing in suggesting right air conditioners to the users based on their requirements. Please strictly stick to the AC assistant context and reply insufficient knowledge in case of any other context.\n",
    "\n",
    "Your objective is to collect the primary feature values from the tool output and fill values for all of the following keys: [\"price\", \"cooling capacity\", \"energy efficiency\", \"comfort\", \"portability\", \"ac type\", \"smart features\"].\n",
    "\n",
    "####\n",
    "Here you can find the instructions on how to collect and fill the values based on user requirements:\n",
    "- Infer the requirements from tools response, strictly no extra text\n",
    "- Give a brief and interesting summary about the filled requirements before asking the follow-up questions and please avoid repetitions\n",
    "- Except price that can contain a numerical value, other keys are supposed to be filled with the above mentioned values in\\\n",
    "    {{\"price\": \"-\", \"cooling capacity\": \"-\", \"energy efficiency\": \"-\", \"comfort\": \"-\", \"portability\": \"-\", \"ac type\": \"-\", \"smart features\": \"-\"}} based on user\"s input\n",
    "- Never show the dictionary with key-value pairs inferred in your response to the user at any cost\n",
    "- Strictly do not fill any value by yourself, unless user is unsure or asking you to decide on the answer to your question.\n",
    "- Do not append any prefix i.e. assistant: or user: in the response.\n",
    "####\n",
    "\n",
    "####\n",
    "Here are the chain of thoughts to fill the values in json:\n",
    "**Thought-1** You have to ask questions one by one nd understand the user's requirements, strictly analyze the tool output to match one or more keys\n",
    "Be intelligent enough to smartly match context of the keys to the specifications mentioned by the user which got extracted through tool response\n",
    "otherwise ask relevant questions in random order to fill in the missing values of [\"price\", \"cooling capacity\", \"energy efficiency\", \"comfort\", \"portability\", \"ac type\", \"smart features\"]\n",
    "Remember the above mentioned instructions when asking the questions\n",
    "Please move forward to the next thought only if you have collected the necessary values\n",
    "**Thought-2** Now you have to fill rest of the keys which are not collected in the previous step\n",
    "Ask clarifying questions to fill all the remaining keys\n",
    "Fill the rest of the keys before going to the next thought\n",
    "**Thought-3** Please ensure the updated values for all the keys are correct\n",
    "If you are not sure about any of the values, ask clarifying questions till you fill all the values correct\n",
    "####\n",
    "\n",
    "***Strictly welcome the user only ONCE without questions and wait till the interaction.***\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca5ee4",
   "metadata": {},
   "source": [
    "### Stage 1 Intent clarification, extraction and confirmation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63333f",
   "metadata": {},
   "source": [
    "##### Intent confirmation system message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE1_SYSTEM_MESSAGE = \"\"\"\n",
    "You have a special skill in for details in the given data and you are expected to return a single word as output.\n",
    "\n",
    "Your task is find whether values for all the keys are filled or not: {{'price', 'cooling capacity', 'energy efficiency', 'comfort', 'portability', 'ac type', 'smart features'}}.\n",
    "Except price which contains a numerical value, all the other values are supposed to have one of the allowed values such as: ['essential', 'standard', 'premium'].\n",
    "\n",
    "Here are the examples of sample input and the corresponding output you are supposed to return\n",
    "\n",
    "####\n",
    "Example1:\n",
    "***input***: \n",
    "- price: 50000 rupees\n",
    "- cooling capacity: standard\n",
    "- energy efficiency: premium\n",
    "- comfort: premium\n",
    "- portability: essential\n",
    "- ac type: standard\n",
    "- smart features: premium\n",
    "***output***: 'Yes'\n",
    "####\n",
    "\n",
    "####\n",
    "please keep the following instructions in mind before the return\n",
    "- Analyze the given input for all keys, if all values are filled without an empty value '-' then strictly return 'Yes'\n",
    "- Don't introduce any space surrounding the single word response and no additional text in the response\n",
    "- Don't add your thoughts or reasoning in the response\n",
    "- Strictly return only a single word either \"Yes\" or \"No\"\n",
    "####\n",
    "\n",
    "*** Please think about your answer carefully before returning the single word response.***\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef881aa",
   "metadata": {},
   "source": [
    "##### Classify the user requirements into categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE1_CLASSIFY_VALUES_SYSTEM_MESSAGE=\"\"\"\n",
    "You are an AC shopping assistant that classifies user preferences into categories.\n",
    "\n",
    "####\n",
    "***Required keys***: \n",
    "- price\n",
    "- cooling capacity\n",
    "- energy efficiency\n",
    "- comfort\n",
    "- portability\n",
    "- ac type\n",
    "- smart features\n",
    "####\n",
    "\n",
    "####\n",
    "***Instructions***:\n",
    "1. If the input dictionary is missing any of the above keys, do not classify yet.\n",
    "2. Only when **all keys are present**, strictly classify the complete preferences into one of three categories except price can contain numerical value: \n",
    "  [\"essential\", \"standard\", \"premium\"]. Strictly return the classified results.\n",
    "3. Output must be a JSON dictionary with the same keys, but each value replaced with one of the categories.\n",
    "####\n",
    "\n",
    "####\n",
    "***Example input***:\n",
    "{{\n",
    "  \"price\": \"65000 rupees\",\n",
    "  \"cooling capacity\": \"2 tons\",\n",
    "  \"energy efficiency\": \"high efficiency\",\n",
    "  \"comfort\": \"less noisy\",\n",
    "  \"portability\": \"fixed\",\n",
    "  \"ac type\": \"split\",\n",
    "  \"smart reatures\": \"wifi control\"\n",
    "}}\n",
    "\n",
    "***Example output:***\n",
    "{{\n",
    "  \"price\": \"65000\",\n",
    "  \"cooling capacity\": \"premium\",\n",
    "  \"energy efficiency\": \"premium\",\n",
    "  \"comfort\": \"standard\",\n",
    "  \"portability\": \"essential\",\n",
    "  \"ac type\": \"standard\",\n",
    "  \"smart features\": \"premium\"\n",
    "}}\n",
    "####\n",
    "\n",
    "Here is your input: {0}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51303f",
   "metadata": {},
   "source": [
    "##### Extract dictionary from string that contains user dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_DICT_SYS_MSG=\"\"\"\n",
    "You are a specialist in extracting a dictionary of key value pairs in JSON format from the given string as per the requirements.\n",
    "\n",
    "####\n",
    "Here are the instructions to extract the key value pairs from the dictionary\n",
    "- You are intelligent enought to extract the keys and values specified in the previous step\n",
    "- Extract the keys: ['price', 'cooling capacity', 'energy efficiency', 'comfort', 'portability', 'ac type', 'smart_features']. and their respective values that are supposed to be one of the three values such as: ['essential', 'standard', 'premium'] except for price from the given input below\n",
    "- Strictly do not extract any other keys or values that are not specified above\n",
    "####\n",
    "\n",
    "####\n",
    "Here are the examples of input and output pairs for the task in hand\n",
    "**Example 1**\n",
    "input:\n",
    "Your requirements are converted into the following \n",
    "- price: 45000 rupees\n",
    "- cooling capacity: standard\n",
    "- energy efficiency: premium\n",
    "- comfort: premium\n",
    "- portability: essential\n",
    "- ac type: standard\n",
    "- smart features: premium\n",
    "output: {{\"price\": \"50000\", \"cooling capacity\": \"standard\", \"energy efficiency\": \"premium\", \"comfort\": \"premium\", \"portability\": \"essential\", \"ac type\": \"standard\", \"smart features\": \"premium\" }}\n",
    "**Example 2**\n",
    "input: I think i captured all the requirements based on user's preferences\\n\n",
    "{{\"price\": \"50000 rupees\", \"cooling capacity\": \"standard\", \"energy efficiency\": \"premium\", \"comfort\": \"premium\", \"portability\": \"essential\", \"ac type\": \"standard\", \"smart features\": \"premium\" }}\n",
    "output: {{\"price\": \"50000\", \"cooling capacity\": \"standard\", \"energy efficiency\": \"premium\", \"comfort\": \"premium\", \"portability\": \"essential\", \"ac type\": \"standard\", \"smart features\": \"premium\" }}\n",
    "**Example 3**\n",
    "input: Your requirements are converted into the following\\n\n",
    "price - 25000 rupees\n",
    "cooling capacity - standard\n",
    "energy efficiency - premium\n",
    "comfort - essential\n",
    "portability - essential\n",
    "ac type - premium\n",
    "smart features - premium\n",
    "output: {{\"price\": \"50000\", \"cooling capacity\": \"standard\", \"energy efficiency\": \"premium\", \"comfort\": \"essential\", \"portability\": \"essential\", \"ac type\": \"premium\", \"smart features\": \"premium\" }}\n",
    "####\n",
    "\n",
    "Strictly output the dictionary in JSON format by extracting the values: ['essential', 'standard', 'premium'] of all the keys ['price', 'cooling capacity', 'energy efficiency', 'comfort', 'portability', 'ac type', 'smart_features'].\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b2ad43",
   "metadata": {},
   "source": [
    "##### Function tool dictionary to extract feature specification from user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad81a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE1_FUNCTION_TOOL={\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"extract_features\",\n",
    "        \"description\": \"From user's input, extract only the features that are explicitly mentioned or updated and strictly do not return key with an empty value which is '-'\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"price\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The standard price, including currency e.g., '45000 INR', '35000 rupees', '25,000', '40000'\", \n",
    "                },\n",
    "                \"cooling capacity\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Two possible inputs here 1. AC's cooling capacity as provided e.g., '1.5 tons capacity', '1.5', '1.5 ton', 2. if room size is given, please convert it to tons e.g., '550 square feet' is approximately equal to '1 ton'\", \n",
    "                },\n",
    "                \"energy efficiency\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Energy efficiency requirements such as 'standard', 'high efficiency', 'inverter'\", \n",
    "                },\n",
    "                \"portability\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Portability factor such as 'portable', 'easy to move', 'fixed', 'for my own house'\", \n",
    "                },\n",
    "                \"ac type\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Type like 'split', 'window', 'central', 'portable ac'.\", \n",
    "                },\n",
    "                \"smart features\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Smart features like 'WiFi', 'voice assistant', 'app remote'\", \n",
    "                },\n",
    "            },\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749ffce",
   "metadata": {},
   "source": [
    "##### Extract features tool choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE1_FUNCTION_TOOL_CHOICE={\n",
    "    \"type\": \"function\", \n",
    "    \"function\": {\"name\": \"extract_features\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df50239",
   "metadata": {},
   "source": [
    "### Stage 2 - Product extraction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE2_SYSTEM_MESSAGE=\"\"\"\n",
    "You are an expert air conditioner data classifier specializing in extracting primary key features and classify it based on the requirements.\n",
    "\n",
    "####\n",
    "To analyze the air conditioner features, please follow the chain of thoughts like mentioned below:\n",
    "Thought 1: Be smart in extracting product's primary features from the description: {0}\n",
    "Thought 2: Fill the extracted features into values: {{\"cooling capacity\": \"Cooling capacity depends on room size\",\"energy efficiency\": \"How efficient in saving energy.\", \"comfort\": \"Lowest noise is the premium feature\", \"portability\": \"Portable or fixed.\", \"ac type\": \"Type of AC\",\"smart features\": \"Smart features inclusion\"}}.\n",
    "Thought 3: Classify each value {{\"cooling capacity\": \"Cooling capacity depends on room size\",\"energy efficiency\": \"How efficient in saving energy.\", \"comfort\": \"Lowest noise is the premium feature\", \"portability\": \"Portable or fixed.\", \"ac type\": \"Type of AC\",\"smart features\": \"Smart features inclusion\"}} into: [\"essential\", \"standard\", \"premium\"] based on the following instructions:\n",
    "####\n",
    "\n",
    "####\n",
    "**cooling capacity**\n",
    "- essential: << if the value is in between 0.8 ton to 1.0 ton >>\n",
    "- standard: << if the value is in between 1.2 to 1.5 tons >>\n",
    "- premium: << if the value is in between 1.8 to 3 tons >>\n",
    "\n",
    "**energy efficiency**\n",
    "- essential: << Allows to have 3 star / fixed-speed >> \n",
    "- standard: << Prefers to have inverter with 4 star rating >>\n",
    "- premium: << Requires inverter with 5 star with high ISEER >>\n",
    "\n",
    "**comfort**\n",
    "- essential: << Any indoor noise level is acceptable; no noise constraint applied. This tier prioritizes availability over comfort. >> \n",
    "- standard: << Quieter home ranges while excluding the louder end of window/portable units. >>\n",
    "- premium: << Prefer indoor less than or equal to 38dB; this expects quiet indoor experiences and favours split or quietest window units  >>\n",
    "\n",
    "**portability**\n",
    "- essential: << Any fixed or movable type is allowed >> \n",
    "- standard: << Allow Split if installation feasible, else Window which are semi portable >>\n",
    "- premium: << Prioritize easily portable units with no outdoor unit Portable or Window (no outdoor unit). If portability is high priority, prefer portable, if noise matters too prefer window. >>\n",
    "\n",
    "**ac type**\n",
    "- essential: << any type allowed >>\n",
    "- standard: << allow Split if installation feasible, else Window >>\n",
    "- premium: << prioritize no-outdoor-unit designs Portable or Window (no outdoor unit) enables easy relocation; choose window over portable when quiet operation is priority.>>\n",
    "\n",
    "**smart features**\n",
    "- essential: << Any type allowed here >>\n",
    "- standard: << Basic remote control feature is included >>\n",
    "- premium: << Supports WiFi, voice and app control. >>\n",
    "\n",
    "####\n",
    "\n",
    "####\n",
    "Input descriptions will be provided like below and strictly output JSON format.\n",
    "**input 1**: Panasonic 1.5 T Wi-Fi Inverter Split is energy-efficient and quiet (~38 dB), with smart voice/app control and 7-in-1 convertible modes—perfect for ~17 m² rooms.\n",
    "**output 1**: {{\"cooling capacity\": \"standard\", \"energy efficiency\": \"premium\", \"comfort\": \"premium\", \"portability\": \"premium\", \"ac type\": \"premium\", \"smart features\": \"premium\"}}\n",
    "**input 2**: Blue Star PC12DB is a compact 1 T portable AC with castor wheels, hydrophilic gold fins, and antibacterial silver coating—excellent for renters.\n",
    "**output 2**: {{\"cooling capacity\": \"essential\", \"energy efficiency\": \"essential\", \"comfort\": \"essential\", \"portability\": \"premium\", \"ac type\": \"standard\", \"smart features\": \"essential\"}}\n",
    "####\n",
    "\n",
    "### Strictly do not add any other text in values of JSON dictionary other than: [\"essential\", \"standard\", \"premium\"]. ###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d630d",
   "metadata": {},
   "source": [
    "### Stage 3 Product recommendations layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE3_SYSTEM_MESSAGE=\"\"\"\n",
    "Act as an Air Conditioner product specialist and resolve user queries exclusively on the recommended products catalog: {0}\n",
    "\n",
    "Start with a brief summary of each product in the following format in a plain text without any separators like '***' or '<>':\n",
    "1. ***AC brand name***: ***Major specifications of the air conditioner***, ***Price in Rs***\n",
    "2. ***AC brand name***: ***Major specifications of the air conditioner***, ***Price in Rs***\n",
    "\n",
    "####\n",
    "Follow the steps below internally when answering the user query: \n",
    "- Step 1: Very carefully compare the user input to catalog  given above for relevance\n",
    "- Step 2: If relevant, answer the query by using the product knowledge; otherwise, return insufficient knowledge on the topic answer\n",
    "####\n",
    "\n",
    "***Strictly display the product recommendations once, then wait for the user's input.***\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647b9d0",
   "metadata": {},
   "source": [
    "## 7. Base class - Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Application:\n",
    "    \"\"\"Base class where pipeline gets injected and accessed in main file.\"\"\"\n",
    "\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f804ee",
   "metadata": {},
   "source": [
    "## 8. Dependency injection module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86f265",
   "metadata": {},
   "source": [
    "##### Stage Container module where all the stages factory methods are created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b134856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dependency_injector import providers\n",
    "\n",
    "class StagesContainer:\n",
    "    \"\"\"Invoking Factory to create instances of the stages, separate instances created and chat models are injected here\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, moderation_model, shared_chat_model, stage2_chat_model, stage3_chat_model\n",
    "    ):\n",
    "        self.stage0 = providers.Factory(\n",
    "            IntializeConversation,\n",
    "            chat_model=shared_chat_model,\n",
    "            system_message=STAGE0_SYSTEM_MESSAGE,\n",
    "        )\n",
    "\n",
    "        self.stage1 = providers.Factory(\n",
    "            IntentClarityAndConfirmation,\n",
    "            chat_model=shared_chat_model,\n",
    "            classify_values_system_message=STAGE1_CLASSIFY_VALUES_SYSTEM_MESSAGE,\n",
    "            intent_confirmation_system_message=STAGE1_SYSTEM_MESSAGE,\n",
    "            extract_dict_system_message=EXTRACT_DICT_SYS_MSG,\n",
    "            function_tool=STAGE1_FUNCTION_TOOL,\n",
    "            function_tool_choice=STAGE1_FUNCTION_TOOL_CHOICE,\n",
    "        )\n",
    "\n",
    "        self.stage2 = providers.Factory(\n",
    "            ProductExtractionAndMapping,\n",
    "            chat_model=stage2_chat_model,\n",
    "            system_message=STAGE2_SYSTEM_MESSAGE,\n",
    "        )\n",
    "        \n",
    "        self.stage3 = providers.Factory(\n",
    "            ProductRecommendations,\n",
    "            chat_model=stage3_chat_model,\n",
    "            system_message=STAGE3_SYSTEM_MESSAGE,\n",
    "        )\n",
    "\n",
    "        self.pipeline = providers.Factory(\n",
    "            Pipeline,\n",
    "            moderation_model,\n",
    "            self.stage0,\n",
    "            self.stage1,\n",
    "            self.stage2,\n",
    "            self.stage3,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f131d1a",
   "metadata": {},
   "source": [
    "##### Model Container module that creates instances related to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d03fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dependency_injector import providers\n",
    "\n",
    "class ModelContainer:\n",
    "    \"\"\"Invoking Factory to create instances of the ChatModel, Moderation classes\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        __DEFAULT_MODEL = \"gpt-4o-mini\"\n",
    "        __MAX_TOKENS = 500\n",
    "        __TEMPERATURE = 0\n",
    "        __NO_OF_CHOICES = 1\n",
    "        __SEED = 7248\n",
    "\n",
    "        common_kwargs = {\n",
    "            \"model\": config.model() or __DEFAULT_MODEL,\n",
    "            \"max_tokens\": config.max_tokens() or __MAX_TOKENS,\n",
    "            \"temperature\": config.temperature() or __TEMPERATURE,\n",
    "            \"no_of_choices\": config.no_of_choices() or __NO_OF_CHOICES,\n",
    "            \"seed\": config.no_of_choices() or __SEED,\n",
    "        }\n",
    "\n",
    "        # Shared chat model is used on both stage 0 (initialize conversation) and stage 1\n",
    "        self.shared_chat_model = providers.Singleton(OpenAIChatModel, **common_kwargs)\n",
    "        # Independent chat model is used on both stage 2 and stage 3\n",
    "        self.stage2_chat_model = providers.Factory(OpenAIChatModel, **common_kwargs)\n",
    "        self.stage3_chat_model = providers.Factory(OpenAIChatModel, **common_kwargs)\n",
    "        self.moderation_model = providers.Factory(ModerationModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509434b7",
   "metadata": {},
   "source": [
    "##### Main container module that creates instances for the pipeline and injects it into application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dependency_injector import containers, providers\n",
    "\n",
    "class Container(containers.DeclarativeContainer):\n",
    "    \"\"\"\n",
    "    Main DI container that is responsible for connecting both model and stages here.\n",
    "\n",
    "    Application class gets created created and the pipeline is injected to access it in the main file.\n",
    "    \"\"\"\n",
    "\n",
    "    config = providers.Configuration()\n",
    "\n",
    "    model_container = ModelContainer(config=config)\n",
    "    stages_container = StagesContainer(\n",
    "        moderation_model=model_container.moderation_model,\n",
    "        shared_chat_model=model_container.shared_chat_model,\n",
    "        stage2_chat_model=model_container.stage2_chat_model,\n",
    "        stage3_chat_model=model_container.stage3_chat_model,\n",
    "    )\n",
    "    pipeline = providers.Factory(\n",
    "        Pipeline,\n",
    "        moderation_client=model_container.moderation_model,\n",
    "        stage0=stages_container.stage0,\n",
    "        stage1=stages_container.stage1,\n",
    "        stage2=stages_container.stage2,\n",
    "        stage3=stages_container.stage3,\n",
    "    )\n",
    "\n",
    "    application = providers.Factory(Application, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87185b",
   "metadata": {},
   "source": [
    "## 9. Front-end setup (Flask + static & templates download)\n",
    "\n",
    "Flask setup and downloading the relevant html and css files into local folder for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4bd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install flask for web application \n",
    "!pip install flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d62988",
   "metadata": {},
   "source": [
    "##### Downloads the **css** files into **static/css/styles.css** path, and **send.png** file to **static** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5123cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c069b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "# make static/css/ folder\n",
    "os.makedirs(\"static/css/\", exist_ok=True)\n",
    "\n",
    "# png file download\n",
    "file_id_png = \"1cMT0qnOiNg86fntMSovMfXzwJ3J_Kw0o\"\n",
    "url_png = f\"https://drive.google.com/uc?id={file_id_png}\"\n",
    "png_file_path = os.path.join(\"static\", \"send.png\")\n",
    "\n",
    "gdown.download(url_png, png_file_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a050282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# css file download\n",
    "file_id_css = \"1KRDhLYCVEmgLt_NIlZsOMQ31FatqqJlZ\"\n",
    "url_css = f\"https://drive.google.com/uc?id={file_id_css}\"\n",
    "css_file_path = os.path.join(\"static\", \"css\", \"styles.css\")\n",
    "gdown.download(url_css, css_file_path, quiet=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9ceb3",
   "metadata": {},
   "source": [
    "##### Downloads the **html** files into **templates/index_chat.html** directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"templates\", exist_ok=True)\n",
    "file_id_html = \"1JUggl7J4O0rz-JSFFp_KGswkp0L0ZJRj\"\n",
    "url_html = f\"https://drive.google.com/uc?id={file_id_html}\"\n",
    "html_file_path = os.path.join(\"templates\", \"index_chat.html\")\n",
    "\n",
    "gdown.download(url_html, html_file_path, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0855b6",
   "metadata": {},
   "source": [
    "## 10. Main application (Entry-point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966059e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from flask import (Flask, Response, redirect, render_template, request,\n",
    "                   stream_with_context, url_for)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "container = Container()\n",
    "application = container.application()\n",
    "\n",
    "\n",
    "def initialize_conversation() -> str:\n",
    "    # Run Initialize conversation\n",
    "    initial_conversation = application.pipeline.run_stage0()\n",
    "    return initial_conversation.response\n",
    "\n",
    "\n",
    "conversation = []\n",
    "top_products = None\n",
    "conversation.append({\"assistant\": initialize_conversation()[0]})\n",
    "\n",
    "\n",
    "@app.route(\"/\", endpoint=\"home_route_endpoint\")\n",
    "def home_route():\n",
    "    global conversation, top_products\n",
    "    return render_template(\"index_chat.html\", conversations=conversation)\n",
    "\n",
    "\n",
    "@app.route(\"/end_conv\", methods=[\"POST\"], endpoint=\"end_route_endpoint\")\n",
    "def end_conv_route():\n",
    "    global conversation, top_products\n",
    "    conversation = []\n",
    "    top_products = None\n",
    "    application.pipeline.clear_messages()\n",
    "    conversation.append({\"assistant\": initialize_conversation()[0]})\n",
    "    return redirect(url_for(\"home_route_endpoint\"))\n",
    "\n",
    "\n",
    "@app.route(\"/chat_stream\", methods=[\"POST\"])\n",
    "def chat_stream_route():\n",
    "    global conversation, top_products\n",
    "    user_input = request.form[\"user_input_message\"]\n",
    "    conversation.append({\"user\": user_input})\n",
    "\n",
    "    @stream_with_context\n",
    "    def generate():\n",
    "        global top_products\n",
    "\n",
    "        def send(role, text, event=\"message\"):\n",
    "            payload = json.dumps({\"role\": role, \"text\": text})\n",
    "            return f\"event:{event}\\ndata:{payload}\\n\\n\"\n",
    "\n",
    "        yield send(\"user\", user_input)\n",
    "\n",
    "        try:\n",
    "            if top_products is None:\n",
    "                # Run Stage 1\n",
    "                stage1 = application.pipeline.run_stage1(user_input=user_input)\n",
    "\n",
    "                if stage1.intent_confirmation.strip().lower() == \"yes\":\n",
    "                    stage2 = application.pipeline.run_stage2(\n",
    "                        user_requirement=stage1.user_requirements\n",
    "                    )\n",
    "                    conversation.append({\"assistant\": stage1.response})\n",
    "\n",
    "                    # Handle empty recommendataions\n",
    "                    if not stage2.recommendations:\n",
    "                        yield send(\n",
    "                            \"assistant\",\n",
    "                            \"Sorry we do not have AC's that match your requirements. Connecting you to a human expert.\",\n",
    "                        )\n",
    "                        yield \"event:end\\ndata:{}\\n\\n\"\n",
    "                        return\n",
    "\n",
    "                    # Run Stage 3 with an initial conversation\n",
    "                    stage3_response = application.pipeline.run_stage3(\n",
    "                        recommendations=stage2.recommendations\n",
    "                    )\n",
    "\n",
    "                    conversation.append(\n",
    "                        {\"assistant\": \"\\n\".join(stage3_response.response)}\n",
    "                    )\n",
    "                    top_products = stage2.recommendations\n",
    "                    yield send(\"assistant\", \"\\n\".join(stage3_response.response))\n",
    "                else:\n",
    "                    conversation.append({\"assistant\": stage1.response})\n",
    "                    yield send(\"assistant\", stage1.response)\n",
    "            else:\n",
    "                # Continue running Stage 3 to resume helping the customer\n",
    "                stage3_continue_response = application.pipeline.continue_stage3(\n",
    "                    user_input\n",
    "                )\n",
    "                response = stage3_continue_response.response\n",
    "                yield send(\"assistant\", response)\n",
    "                conversation.append({\"assistant\": response})\n",
    "\n",
    "        except ModerationException as te:\n",
    "            yield send(\"assistant\", te.message)\n",
    "            yield \"event:block\\ndata:{}\\n\\n\"\n",
    "            return\n",
    "        except Exception as e:\n",
    "            yield send(\"assistant\", str(e))\n",
    "            yield \"event:block\\ndata:{}\\n\\n\"\n",
    "            return\n",
    "\n",
    "        # To finish the stream\n",
    "        yield \"event:end\\ndata:{}\\n\\n\"\n",
    "\n",
    "    return Response(generate(), mimetype=\"text/event-stream\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    app.run(debug=True, threaded=True, use_reloader=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c345a19",
   "metadata": {},
   "source": [
    "## 11. Unit testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2103df",
   "metadata": {},
   "source": [
    "### Compare products unit testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0547c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "\n",
    "def test_no_requirements_met():\n",
    "    user = {\n",
    "        \"cooling capacity\": \"standard\",\n",
    "        \"energy efficiency\": \"standard\",\n",
    "        \"comfort\": \"premium\",\n",
    "        \"portability\": \"premium\",\n",
    "        \"ac type\": \"premium\",\n",
    "        \"smart features\": \"premium\",\n",
    "    }\n",
    "    dataset = {\n",
    "        \"cooling capacity\": \"essential\",\n",
    "        \"energy efficiency\": \"essential\",\n",
    "        \"comfort\": \"essential\",\n",
    "        \"portability\": \"essential\",\n",
    "        \"ac type\": \"standard\",\n",
    "        \"smart features\": \"essential\",\n",
    "    }\n",
    "\n",
    "    assert compare_products(user_requirements=user, from_dataset=dataset) == 0\n",
    "\n",
    "def test_all_requirements_met():\n",
    "    user = {\n",
    "        \"cooling capacity\": \"essential\",\n",
    "        \"energy efficiency\": \"essential\",\n",
    "        \"comfort\": \"essential\",\n",
    "        \"portability\": \"essential\",\n",
    "        \"ac type\": \"standard\",\n",
    "        \"smart features\": \"essential\",\n",
    "    }\n",
    "    dataset = {\n",
    "        \"cooling capacity\": \"standard\",\n",
    "        \"energy efficiency\": \"standard\",\n",
    "        \"comfort\": \"premium\",\n",
    "        \"portability\": \"essential\",\n",
    "        \"ac type\": \"standard\",\n",
    "        \"smart features\": \"premium\",\n",
    "    }\n",
    "\n",
    "    assert compare_products(user_requirements=user, from_dataset=dataset) == 6\n",
    "\n",
    "def test_four_requirements_met():\n",
    "    user = {\n",
    "        \"cooling capacity\": \"essential\",\n",
    "        \"energy efficiency\": \"essential\",\n",
    "        \"comfort\": \"premium\",\n",
    "        \"portability\": \"essential\",\n",
    "        \"ac type\": \"premium\",\n",
    "        \"smart features\": \"essential\",\n",
    "    }\n",
    "    dataset = {\n",
    "        \"cooling capacity\": \"standard\",\n",
    "        \"energy efficiency\": \"standard\",\n",
    "        \"comfort\": \"essential\",\n",
    "        \"portability\": \"essential\",\n",
    "        \"ac type\": \"standard\",\n",
    "        \"smart features\": \"premium\",\n",
    "    }\n",
    "\n",
    "    assert compare_products(user_requirements=user, from_dataset=dataset) == 4\n",
    "\n",
    "def test_missing_keys_in_dataset():\n",
    "    user = {\"cooling capacity\": \"standard\", \"energy efficiency\": \"standard\"}\n",
    "    dataset = {\"cooling capacity\": \"standard\"}\n",
    "\n",
    "    assert compare_products(user_requirements=user, from_dataset=dataset) == 1\n",
    "\n",
    "def test_missing_dataset():\n",
    "    user = {\n",
    "        \"cooling capacity\": \"essential\",\n",
    "        \"energy efficiency\": \"essential\",\n",
    "        \"comfort\": \"premium\",\n",
    "        \"portability\": \"essential\",\n",
    "        \"ac type\": \"premium\",\n",
    "        \"smart features\": \"essential\",\n",
    "    }\n",
    "    dataset = {}\n",
    "\n",
    "    assert compare_products(user_requirements=user, from_dataset=dataset) == 0\n",
    "\n",
    "\n",
    "def test_missing_both_user_requirement_and_in_dataset():\n",
    "    user = {}\n",
    "    dataset = {}\n",
    "\n",
    "    assert compare_products(user_requirements=user, from_dataset=dataset) == 0\n",
    "\n",
    "\n",
    "def test_missing_values_in_user():\n",
    "    user = {\n",
    "        \"cooling capacity\": \"\",\n",
    "        \"energy efficiency\": \"\",\n",
    "        \"comfort\": \"\",\n",
    "        \"portability\": \"essential\",\n",
    "        \"ac type\": \"premium\",\n",
    "        \"smart features\": \"essential\",\n",
    "    }\n",
    "    dataset = {\n",
    "        \"cooling capacity\": \"essential\",\n",
    "        \"energy efficiency\": \"essential\",\n",
    "        \"comfort\": \"premium\",\n",
    "        \"portability\": \"standard\",\n",
    "        \"ac type\": \"standard\",\n",
    "        \"smart features\": \"essential\",\n",
    "    }\n",
    "\n",
    "    assert compare_products(user_requirements=user, from_dataset=dataset) == 5\n",
    "\n",
    "\n",
    "def test_missing_values_in_dataset():\n",
    "    user = {\n",
    "        \"cooling capacity\": \"essential\",\n",
    "        \"energy efficiency\": \"essential\",\n",
    "        \"comfort\": \"premium\",\n",
    "        \"portability\": \"standard\",\n",
    "        \"ac type\": \"standard\",\n",
    "        \"smart features\": \"essential\",\n",
    "    }\n",
    "    dataset = {\n",
    "        \"cooling capacity\": \"\",\n",
    "        \"energy efficiency\": \"\",\n",
    "        \"comfort\": \"\",\n",
    "        \"portability\": \"essential\",\n",
    "        \"ac type\": \"premium\",\n",
    "        \"smart features\": \"essential\",\n",
    "    }\n",
    "\n",
    "    assert compare_products(user_requirements=user, from_dataset=dataset) == 2\n",
    "\n",
    "\n",
    "def test_unknown_values_in_dataset():\n",
    "    user = {\n",
    "        \"cooling capacity\": \"standard\",\n",
    "        \"energy efficiency\": \"standard\",\n",
    "        \"portability\": \"unknown\",\n",
    "    }\n",
    "    dataset = {\n",
    "        \"cooling capacity\": \"premium\",\n",
    "        \"energy efficiency\": \"unknown\",\n",
    "        \"portability\": \"unknown\",\n",
    "    }\n",
    "\n",
    "    assert compare_products(user_requirements=user, from_dataset=dataset) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3103e3",
   "metadata": {},
   "source": [
    "### Stage 0 Initialize conversation test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import create_autospec\n",
    "\n",
    "\n",
    "def test_stage_0_calls_initialize_conversation_with_expected_messages():\n",
    "    chat_model = create_autospec(ChatModel, instance=True)\n",
    "    system_message = \"\"\"\n",
    "    Act as a smart Air Conditioner recommendation assistant specializing in suggesting right air conditioners to the users based on their requirements. Please strictly stick to the AC assistant context and reply insufficient knowledge in case of any other context.\n",
    "    \"\"\"\n",
    "    FAKE_RESPONSE = \"Welcome! I am a bot assistant to help you on selecting an ac!\"\n",
    "    response = StageZeroResult(FAKE_RESPONSE)\n",
    "    chat_model.get_session_response.return_value = FAKE_RESPONSE\n",
    "\n",
    "    stage_0 = IntializeConversation(\n",
    "        chat_model=chat_model, system_message=system_message\n",
    "    )\n",
    "\n",
    "    actual = stage_0.run()\n",
    "\n",
    "    assert actual == response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18271194",
   "metadata": {},
   "source": [
    "### Stage 1 Intent Clarification and Confirmation test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_chat_model():\n",
    "    return MagicMock()\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def intent_clarity_and_confirmation(mock_chat_model):\n",
    "    return IntentClarityAndConfirmation(\n",
    "        chat_model=mock_chat_model,\n",
    "        classify_values_system_message=\"System message\",\n",
    "        intent_confirmation_system_message=\"System: {model_response}\",\n",
    "        extract_dict_system_message=\"Extract dict system message\",\n",
    "        function_tool={\"tool1\": \"desc\"},\n",
    "        function_tool_choice={\"tool1\": \"testing\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def test_stage1_intent_confirmation_produces_user_requirements(\n",
    "    intent_clarity_and_confirmation, mock_chat_model\n",
    "):\n",
    "    tool_response = [\n",
    "        {\n",
    "            \"tool_calls\": [{\"type\": \"test_tool\", \"function\": {\"arguments\": {}}}],\n",
    "            \"id\": \"tool_id_1\",\n",
    "            \"content\": {\"feature\": \"value\"},\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    main_response = [\"\"]\n",
    "    intent_confirmation = [\"Yes\"]\n",
    "    extracted_dict = {\"feature\": \"value\"}\n",
    "\n",
    "    mock_chat_model.get_session_response.side_effect = [tool_response, main_response]\n",
    "    intent_clarity_and_confirmation._IntentClarityAndConfirmation__intent_confirmation = MagicMock(\n",
    "        return_value=intent_confirmation\n",
    "    )\n",
    "    intent_clarity_and_confirmation._IntentClarityAndConfirmation__extract_dict_from_string = MagicMock(\n",
    "        return_value=extracted_dict\n",
    "    )\n",
    "\n",
    "    result = intent_clarity_and_confirmation.run()\n",
    "\n",
    "    assert isinstance(result, StageOneResult)\n",
    "    assert result.intent_confirmation == \"Yes\"\n",
    "    assert result.response == \"\"\n",
    "    assert result.user_requirements == extracted_dict\n",
    "\n",
    "def test_stage1_with_no_intent_confirmation(\n",
    "    intent_clarity_and_confirmation, mock_chat_model\n",
    "):\n",
    "    tool_response = [\n",
    "        {\n",
    "            \"tool_calls\": [{\"type\": \"test_tool\", \"function\": {\"arguments\": {}}}],\n",
    "            \"id\": \"tool_id_1\",\n",
    "            \"content\": {\"feature\": \"value\"},\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    main_response = [\"\"]\n",
    "    intent_confirmation = [\"No\"]\n",
    "    extracted_dict = {}\n",
    "\n",
    "    mock_chat_model.get_session_response.side_effect = [tool_response, main_response]\n",
    "    intent_clarity_and_confirmation._IntentClarityAndConfirmation__intent_confirmation = MagicMock(\n",
    "        return_value=intent_confirmation\n",
    "    )\n",
    "    intent_clarity_and_confirmation._IntentClarityAndConfirmation__extract_dict_from_string = MagicMock(\n",
    "        return_value=extracted_dict\n",
    "    )\n",
    "\n",
    "    result = intent_clarity_and_confirmation.run()\n",
    "\n",
    "    assert isinstance(result, StageOneResult)\n",
    "    assert result.intent_confirmation == \"No\"\n",
    "    assert result.response == \"\"\n",
    "    assert result.user_requirements == extracted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f0d75",
   "metadata": {},
   "source": [
    "### Stage 2 Product extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e83875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import create_autospec\n",
    "\n",
    "\n",
    "def test_stage_2_calls_product_extraction_with_expected_messages():\n",
    "    chat_model = create_autospec(ChatModel, instance=True)\n",
    "    system_message = \"Extract products and dictionary from map\"\n",
    "    FAKE_RESPONSE = {\n",
    "        \"cooling capacity\": \"standard\",\n",
    "        \"energy efficiency\": \"premium\",\n",
    "        \"comfort\": \"essential\",\n",
    "        \"portability\": \"essential\",\n",
    "        \"ac type\": \"standard\",\n",
    "        \"smart features\": \"premium\",\n",
    "    }\n",
    "    DATABASE_RETRIEVAL = '[{\"brand\":\"Lloyd\",\"model_name\":\"1.2 Ton 4 Star Dual Inverter Split AC\",\"room_size\":14,\"capacity_ton\":1.2,\"energy_efficiency\":4,\"inverter\":\"Yes\",\"operating_range\":\"18\\\\u201345 \\\\u00b0C\",\"installation_required\":\"Yes\",\"delivery_duration\":\"3\\\\u20135 days\",\"ac_type\":\"Split\",\"smart_features\":\"4\\\\u2011Way Swing\",\"noise_levels\":\"41 dB\",\"stars\":3.9,\"price\":44990,\"warranty\":\"2 Years Product, 10 Years Compressor\",\"portability\":\"Fixed\",\"description\":\"Price \\\\u20b944990: Lloyd 1.2 Ton 4 Star Dual Inverter Split AC offers a cooling capacity of 1.2 Ton for ~14 m\\\\u00b2 rooms. Energy efficiency is 4-Star with inverter compression, and portability is \\'Fixed\\' for the Split form factor. Comfort is moderately quiet at around 41 dB. Smart features: 4\\\\u2011Way Swing. Ideal for Indian summers with an operating range of 18\\\\u201345 \\\\u00b0C.\"},{\"brand\":\"Samsung\",\"model_name\":\"1.0 Ton 3 Star Inverter Split AC\",\"room_size\":12,\"capacity_ton\":1.0,\"energy_efficiency\":3,\"inverter\":\"Yes\",\"operating_range\":\"16\\\\u201352 \\\\u00b0C\",\"installation_required\":\"Yes\",\"delivery_duration\":\"3\\\\u20135 days\",\"ac_type\":\"Split\",\"smart_features\":\"Wi\\\\u2011Fi + Voice\",\"noise_levels\":\"41 dB\",\"stars\":4.4,\"price\":43300,\"warranty\":\"2 Years Product, 10 Years Compressor\",\"portability\":\"Fixed\",\"description\":\"Price \\\\u20b943300: Samsung 1.0 Ton 3 Star Inverter Split AC offers a cooling capacity of 1.0 Ton for ~12 m\\\\u00b2 rooms. Energy efficiency is 3-Star with inverter compression, and portability is \\'Fixed\\' for the Split form factor. Comfort is moderately quiet at around 41 dB. Smart features: Wi\\\\u2011Fi + Voice. Ideal for Indian summers with an operating range of 16\\\\u201352 \\\\u00b0C.\"},{\"brand\":\"Panasonic\",\"model_name\":\"1.5 Ton 3 Star Window AC\",\"room_size\":18,\"capacity_ton\":1.5,\"energy_efficiency\":3,\"inverter\":\"No\",\"operating_range\":\"18\\\\u201350 \\\\u00b0C\",\"installation_required\":\"Yes\",\"delivery_duration\":\"4\\\\u20136 days\",\"ac_type\":\"Window\",\"smart_features\":\"Self Clean\",\"noise_levels\":\"52 dB\",\"stars\":4.3,\"price\":34460,\"warranty\":\"1 Year Product, 5 Years Compressor\",\"portability\":\"Semi-portable\",\"description\":\"Price \\\\u20b934460: Panasonic 1.5 Ton 3 Star Window AC offers a cooling capacity of 1.5 Ton for ~18 m\\\\u00b2 rooms. Energy efficiency is 3-Star, and portability is \\'Semi-portable\\' for the Window form factor. Comfort is moderately quiet at around 52 dB. Smart features: Self Clean. Ideal for Indian summers with an operating range of 18\\\\u201350 \\\\u00b0C.\"}]'\n",
    "    fake_response = StageTwoResult(recommendations=DATABASE_RETRIEVAL)\n",
    "\n",
    "    chat_model.preview_response.return_value = FAKE_RESPONSE\n",
    "    stage_2 = ProductExtractionAndMapping(\n",
    "        chat_model=chat_model, system_message=system_message\n",
    "    )\n",
    "\n",
    "    actual = stage_2.run(\n",
    "        user_requirement={\n",
    "            \"price\": \"45000\",\n",
    "            \"cooling capacity\": \"standard\",\n",
    "            \"energy efficiency\": \"premium\",\n",
    "            \"comfort\": \"essential\",\n",
    "            \"portability\": \"essential\",\n",
    "            \"ac type\": \"standard\",\n",
    "            \"smart features\": \"premium\",\n",
    "        }\n",
    "    )\n",
    "    assert actual == fake_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b122b",
   "metadata": {},
   "source": [
    "### Stage 3 Product recommendations test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec09f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import create_autospec\n",
    "\n",
    "\n",
    "def test_stage_3_calls_intent_confirmation_with_expected_messages():\n",
    "    chat_model = create_autospec(ChatModel, instance=True)\n",
    "    system_message = \"Display recommendatation and chat with user.\"\n",
    "    FAKE_RESPONSE = \"lloyd model response\"\n",
    "    fake_response = StageThreeResult(FAKE_RESPONSE)\n",
    "\n",
    "    chat_model.get_session_response.return_value = FAKE_RESPONSE\n",
    "    stage_3 = ProductRecommendations(\n",
    "        chat_model=chat_model, system_message=system_message\n",
    "    )\n",
    "\n",
    "    actual = stage_3.run()\n",
    "    assert actual == fake_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239df1d",
   "metadata": {},
   "source": [
    "##### Pytest can be run with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846412b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
